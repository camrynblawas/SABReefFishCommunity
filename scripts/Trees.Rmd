#Load libraries
```{r}
library(tidyverse)
library(here)
library(ggmap)
library(ggpmisc)
library(stringr)
library(sf)
library(sp)
library(spatialEco)
library(rnaturalearth)
library(taxize)
#check here
here::here()

library(tidymodels)
library(tidyflow)
library(rpart.plot)
library(vip)
library(baguette)
library(ranger)

my_theme <- function() {theme_bw() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), plot.title = element_text(size=12, face= "bold", colour= "black"), axis.title.x = element_text(size=12, face="bold", colour = "black"), axis.title.y = element_text(size=12, face="bold", colour = "black"))}
```

#Get clean data
```{r}
fishdf <- read.csv(here("data", "processeddata", "RFS_AbundanceBiomassDepthLength.csv"))
fishdf$date <- as.Date(fishdf$date)
```

We are interested in how the reef fish community has changed over time and want to build a tree based model of what environmental factors drive distributions of fishes

Variables we could include: 
 - Location (Lat/Long)
 - Date
 - Family
 - Length
 - Weight
 - Sex
 - Maturity
 - Depth
 - Temperature (surface/bottom)
 - Salinity (surface/bottom)
 - Dissolved Oxygen (surface/bottom)
 
#Select variables of interest
```{r}
names(fishdf)
#there are significantly fewer rows with maturity descriptions 
fishdf <- fishdf %>% dplyr::select(date, year, month, speciesscientificname, speciescommonname.y, speciestotalweight, tempsurface, tempbottom, salinitysurface, salinitybottom, sdo, bdo, latitudestart, longitudestart, depthstart, family, len1, weight, sexdescription, maturitydescription)
fishdf <- fishdf %>% drop_na(sexdescription) %>% dplyr::filter(sexdescription != "") %>% dplyr::filter(sexdescription != "NOT TAKEN/UNKNOWN")
fishdf <- fishdf %>% drop_na(maturitydescription) %>% dplyr::filter(maturitydescription != "") %>% dplyr::filter(maturitydescription != "UNKNOWN")
```

#Pick out species of interest
```{r}
metrics <- fishdf %>% group_by(speciesscientificname) %>% summarize(abundance = n(), biomass = sum(speciestotalweight, na.rm = TRUE), meansurftemp = mean(tempsurface, na.rm = TRUE), meansurfsal = mean(salinitysurface, na.rm = TRUE), meansurfdo = mean(sdo, na.rm = TRUE))

arrange(metrics, desc(abundance))

#Start with vermillion snapper? "RHOMBOPLITES AURORUBENS"
```

#Model building
Lets say we want to know where the heavy snapper are
```{r}
snapper <- fishdf %>% dplyr::filter(speciesscientificname == "RHOMBOPLITES AURORUBENS")
snapper %>% group_by(maturitydescription) %>% count()
snapperdata <- snapper %>% dplyr::select(year, month, tempsurface, tempbottom, salinitybottom, salinitysurface, sdo, bdo, latitudestart, longitudestart, depthstart, weight, sexdescription, maturitydescription)
snapperdata <- snapperdata %>% dplyr::filter(maturitydescription %in% c("RIPE", "DEVELOPING; VITELLOGENESIS", "RECENT SPAWN; OLD POFs"))

snapperdata <- snapperdata %>% drop_na(weight)
```
#Basic models
```{r}
# Define the decision tree and tell it the the dependent
# variable is continuous ('mode' = 'regression')
mod1 <- set_engine(decision_tree(mode = "regression"), "rpart")

tflow <-
  # Plug the data
  snapperdata %>%
  # Begin the tidyflow
  tidyflow(seed = 23151) %>%
  # Separate the data into training/testing (we are keeping 3/4 of the data for training)
  plug_split(initial_split, prop = 3/4) %>%
  # Plug the formula
  plug_formula(weight ~ .) %>%
  # Plug the model
  plug_model(mod1)

vanilla_fit <- fit(tflow)
tree <- pull_tflow_fit(vanilla_fit)$fit
rpart.plot(tree)

vanilla_fit %>%
  pull_tflow_fit() %>%
  .[['fit']] %>%
  vip() +
  theme_minimal() + labs(title = "Vermillion Snapper Decision Tree Variable Importances", x = "Variable", y = "Importance")
```
Big fish are deeper and more southern


#Random forests
```{r}
# Define the random forest
rf_mod <-
  rand_forest(mode = "regression") %>%
  set_engine("ranger", importance = "impurity")

# Define the `tidyflow` with the random forest model
# and include all variables (including scie_score and read_score)
tflow <-
  snapperdata %>%
  tidyflow(seed = 23151) %>%
  plug_formula(weight ~ .) %>%
  plug_split(initial_split) %>%
  plug_model(rf_mod)

res_rf <- tflow %>% fit()

res_rf %>%
  predict_training() %>%
  rmse(weight, .pred)

res_rf %>%
  pull_tflow_fit() %>%
  .[['fit']] %>%
  vip() +
  theme_minimal() + labs(title = "Vermillion Snapper Random Forest Variable Importances", x = "Variable", y = "Importance")
```
Biggest drivers are depth and location

#Tune random forests
I am going to mess with the "mtry" argument to 1/3 of the amount of variables (about 4), i.e. the number of columns to be used in each split
```{r}
rf_mod <-
  rand_forest(mode = "regression", mtry = 4) %>%
  set_engine("ranger")

res_rf <- tflow %>% replace_model(rf_mod) %>% fit()

res_rf %>%
  predict_training() %>%
  rmse(weight, .pred)

#Predict on testing
rf_rmse <-
  res_rf %>%
  predict_testing() %>%
  rmse(weight, .pred) %>%
  pull(.estimate)
```
RMSE is slightly smaller, with error of 0.11 grams.

Let's use tidyflow's grid_regular to obtain an evenly spaced set of values for trees -- we'll divide the data into training/testing, specify a cross-validation and plug in the random forest and a grid_regular
```{r}
rf_mod <-
  rand_forest(mode = "regression",
              trees = tune()) %>%
  set_engine("ranger")

tflow <-
  snapperdata %>% 
  tidyflow(seed = 2151) %>%
  plug_split(initial_split) %>%
  plug_resample(vfold_cv) %>%
  plug_grid(grid_regular, levels = 10) %>%
  plug_formula(weight ~ .) %>%
  plug_model(rf_mod)

tflow

res <- tflow %>% fit()
res %>% pull_tflow_fit_tuning() %>% autoplot()

final_mod <- res %>% complete_tflow(metric = "rmse")

final_mod %>%
  predict_training() %>%
  rmse(weight, .pred)
```
RMSE is actually slightly worse. Tough.

#xgboost
```{r}
boost_mod <-
  boost_tree(mode = "regression", trees = 500) %>%
  set_engine("xgboost")

tflow <-
  snapperdata %>%
  tidyflow(seed = 51231) %>%
  plug_formula(weight ~ .) %>%
  plug_split(initial_split) %>%
  plug_model(boost_mod)

res_boost <- fit(tflow)

rmse_gb_train <-
  res_boost %>%
  predict_training() %>%
  rmse(weight, .pred)

rmse_gb_train

```
Pretty good improvement in RMSE!

#Compare RF and xgboost
```{r}
gb_rmse <-
  res_boost %>%
  predict_testing() %>%
  rmse(weight, .pred) %>%
  pull(.estimate)

c(
  "Random Forest" = rf_rmse,
  "Extreme Gradient Boosting" = gb_rmse)
```
xgboost performs a little better. Interesting! Let's pick out a few key species and figure out variable importance 


